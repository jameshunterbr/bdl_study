---
title: "Tests of BDL Bias and Variance - v.1a"
author:
  - name: James R. Hunter, D.Sc.
    affiliation: Laboratório de Retrovirologia, UNIFESP
date: last-modified

format: 
  typst: 
    mainfont: "Nunito"
    fontsize: 11pt

editor: visual

execute:
  echo: false
  warning: false
  message: false
bibliography: references.bib
---

## Purpose

This file will test the determination of bias and variance when calculating viral loads and other variables that are subject to detection limits. For HIV-1 patients, the objective of antiretroviral treatment is to make their plasma RNA viral load zero.

However, there is a practical limit below which the assays cannot distinguish between the presence of virus and having no virus (the zero objective). This is called the "detection limit" and patients who achieve this level are said to have a viral load "BDL", below detection limit. One of the most important consequences of exactitude in measuring true lack of the presence of virus is the phrase "undetectable = untransmittable" [@chang2025][@combescure2009]. This frees, at least emotionally, the individual from concern about transmitting the virus to a partner.

There have been various strategies to deal with calculations of viral load in a panel of patients. One has been to eliminate the cases from the calculation, another has been to treat the viral level of those patients who are BDL as 0, assuming they are really without any virus. Still another strategy has been to assign a value equal to the detection limit. Others have used a value of half the detection limit, i.e., half of the difference between the detection limit and zero.

| Name   | Strategy                         | Value Assigned to "BDL" |
|--------|----------------------------------|-------------------------|
| censor | eliminate case                   | no value                |
| zero   | treat as zero                    | 0                       |
| dl     | treat as detection limit         | detection limit         |
| halfdl | treat as half of detection limit | detection limit/2       |

: Table 1: Strategies for Handling BDL Values

This study was inspired by a blog post by Nikolas Siccha, "Lower Limits of Detection or Quantification".[@siccha2025] He develops a Bayesian model to explore the pharmacodynamics of drugs that cause the measurement of interest to fall below detection limits. As it deals with the dynamics over time of a drug's presence and efficacy in a patient, his question is more complex than the question I would like to address here.

::: callout-important
What is the relationship between the true value of viral load below detection limit and the strategies used to estimate it and how does that effect analyses of the viral load of a sample or population?
:::

```{r}

#| label: packages_variables

#| echo: false

pacman::p_load(tidyverse, janitor, ggpubr, ggsci, gt, summarytools)

uchicago <- pal_uchicago(palette = "default")(8)
crimson <- uchicago[7]
gray <- uchicago[2]
blue <- uchicago[5]
green <- uchicago[8]
stats <- c("mean", "sd", "min", "med", "max", "iqr", "cv")

## assumed values
n <- 1000
blmean <- 900
blsd <- 1000
reduct_pct <- .975 # pct reduction in w48
dl <- 50 # detection limit

```

## Method

I will create a panel of 1,000 PLWH artificially with estimates of their viral load at a baseline and after 48 simulated weeks of antiretroviral treatment (ART) when some of them will have viral loads reduced to BDL, which I will define here as 50 copies per mL.[^1] According to a 2009 clinical trial (STARTMRK) of Raltegravir, a 1st generation integrase inhibitor, 86.1% of the RTG arm of naive patients had viral loads BDL by week 48. [@lennox2009] Likewise, a clinical trial (SINGLE) of a second generation integrase inhibitor, Dolutagravir, showed 88% of the DTG arm with viral loads below detection limit by week 48. [@walmsley2013] For my panel, I will use a proportion of 85% of patients achieving true viral loads between 0 and 50 by the 48 week mark. 85% is a bit more conservative than either of the real trials, but consistent with their results.

[^1]: While 50 copies was a common level for detection limit until 2024, since then it has been reduced to 30 or 20 copies depending on the measurement technology used. I am using 50 here as the purpose is to explore the bias and variance of samples rather than to test the detection limit itself.

The baseline viral loads will be determined by calculating 1,000 normal variates from a truncated normal distribution with a mean of `r format(blmean, big.mark = ",")` copies per mL and a standard deviation of `r format(blsd, big.mark = ",")` copies. Since values below detection limit make no sense for this simulation, these values will be removed by using the truncated normal distribution.[@mersmann2023]

The 48 week data will be reduced dramatically (and unrealistically)[^2] by `r 100 * reduct_pct`% and the number of values below 50 copies per mL will be calculated. I will then create a second version substituting the true values below the detection limit with BDL. Using the four strategies outlined in Table 1, I will then compare the bias and variance of each of these strategies for substituting the true value of the viral load. Finally, I will propose a method for imputing values that is commonly applied in studies using the R statistical language.

[^2]: While unrealistic, this large reduction is useful to give us a number of values BDL that can be tested against the 4 strategies.

The flow chart below shows a schematic of the workflow of this study.

![](flowchart%20bdl.png){fig-align="center" width="3.1in"}

## Baseline

```{r}

#| label: determine_cohort_bl
#| echo: false

set.seed(42)

# calculate baseline distribution
# set as integer
bl <- round(truncnorm::rtruncnorm(n, mean = blmean, sd = blsd, a = dl), 0)

# calculate summary stats
descr(bl, stats = stats)

vldf <- tibble(bl)
vldf |> 
  gghistogram(x = "bl",
              bins = 20,
              fill = crimson,
              title = "Viral Load Baseline Values - Simulated",
              add = "mean",
              add.params = list(color = gray, size = 1.2),
              xlab = "Viral Load",
              ylab = "Count",
              ggtheme = theme_bw()) +
  annotate("text", x = 1800, y = 125, label = "Mean of Viral Loads", 
           color = gray)
```

## Week 48 Viral Load

We will calculate the viral load at 48 weeks to be `r 100 * reduct_pct`% less than the original viral load.

```{r}

#| label: 48_weeks

vldf <- vldf |> 
  mutate(w48 = round(bl * (1 - reduct_pct), 0))

num_vals_bdl <- length(vldf$w48[vldf$w48 < 50])

# substitute "bdl" for values less than 50
vldf <- vldf |>
  mutate(w48_bdl = ifelse(w48 < 50, "bdl", w48))

descr(vldf$w48, stats = stats)

# true value of wk48 mean
true_mean <- mean(vldf$w48)

vldf |> 
  gghistogram(x = "w48",
              bins = 20,
              fill = crimson,
              title = "Viral Load Week 48 Values",
              add = "mean",
              add.params = list(color = gray, size = 1.2),
              xlab = "Viral Load",
              ylab = "Count",
              ggtheme = theme_bw()) +
  annotate("text", x = 50, y = 110, label = "Mean of Viral Loads", 
           color = gray)
# tibble to compare true values with strategies
wk48_compare <- vldf |> 
  summarise(
    strategy = "true",
    mean_wk48 = true_mean,
    diff = 0.0,
    sd_wk48 = sd(w48),
    var_wk48 = sd_wk48^2,
    med_wk48 = median(w48),
    num = n(), 
    cv = sd_wk48/mean_wk48)


# function for showing compare week 48 table
compare_strat_table <- function() {
  wk48_compare |> 
  gt() |> 
  cols_label(
    strategy = "Strategy",
    diff = "Diff True Mean",
    mean_wk48 = "Mean",
    sd_wk48 = "Std. Dev.",
    var_wk48 = "Variance",
    med_wk48 = "Median",
    num = "# of Cases",
    cv = "Coeffient Var."
  ) |> 
  fmt_number(
    columns = c(mean_wk48, diff, sd_wk48, var_wk48, cv),
    decimals = 2
  ) |> 
  tab_spanner(
    label = "Week 48 Viral Load",
    columns = c(mean_wk48:num)
  )
}
```

To give you an idea of what we have now, here are the first 10 cases in our dataset. While we know the true values of the viral load, the assay itself will only give us a BDL signal instead of the value. This occurs `r num_vals_bdl` times out of the 1,000 cases. The question is what can we do about it and what does that do to the distribution of viral loads that we are trying to use to make an inference about HIV-1 treatment strategies.

```{r}

#| data_table
#| echo: false

vldf |> 
  slice(1:10) |> 
  gt() |> 
  fmt_number(
    decimals = 0
  ) |> 
  cols_label(
    bl = "Baseline",
    w48 = "True Value",
    w48_bdl = "BDL"
  ) |> 
  tab_spanner(
    label = "Week 48",
    columns = c(w48, w48_bdl)
  )

# store number of bdl's for text below
numbdl <- length(vldf$w48_bdl[vldf$w48_bdl == "bdl"])

vldf |> 
  ggdensity(x = "w48",
            fill = crimson,
            xlab = "Viral Load",
            ylab = "Density",
            title = "Density of True Value Viral Load",
            add = "mean",
            add.params = list(size = 1.2, color = "black"),
            xlim = c(0, 110),
            ggtheme = theme_bw()) +
  annotate("text", x = 40, y = .04, label = "Mean True", color = "black")
```

## Bias and Variance

The measures I am going to use to test the strategies listed at the beginning of this analysis are bias and variance. Bias measures how much the strategy distorts the distribution of true values. I will show this by comparing the mean, median, and standard deviation of the results of the strategies to those of the true values and through a density visualization of the distributions. The variance is rather more straightforward as we can find the statistical variance as determined by the classical formula for this quantity.

## The Four Strategies

### Censorship

In this strategy, which is very commonly applied when laboratories confront results of assays that do not indicate numerical values, simply removes those cases from the study.

When we do that here, we throw away `r numbdl` cases, meaning `r 100 * numbdl/n`% of the cases, which likely contain many other nuggets of useful information. Before we even arrive at judging bias and variance, this reckless disposal of worthwhile information is the principal defect of the strategy. This dataset is not arbitrarily skewed toward having cases with BDL viral load after 48 weeks. As pointed out at the beginning, many studies show results in the range of 85%.

```{r}

#| label: censorship

censordf <- vldf |> 
  filter(w48_bdl != "bdl")

wk48_compare <- wk48_compare |> 
  add_case(strategy = "censored",
           mean_wk48 = mean(censordf$w48),
           diff = mean_wk48 - true_mean,
           sd_wk48 = sd(censordf$w48),
           var_wk48 = sd_wk48^2,
           med_wk48 = median(censordf$w48),
           num = nrow(censordf), 
           cv = sd_wk48/mean_wk48)

compare_strat_table()

ggcensor <- tibble(true = vldf$w48,
                   censored = c(censordf$w48, rep(NA, num_vals_bdl))) |> 
  pivot_longer(cols = everything(), names_to = "Strategy", values_to = "vl")|> 
  ggdensity(x = "vl",
            palette = "uchicago",
            color = "Strategy",
            fill = "Strategy",
            xlab = "Viral Load",
            ylab = "Density",
            title = "Density of Censored Viral Load",
            xlim = c(0, 110),
            ggtheme = theme_bw()) +
  geom_vline(xintercept = mean(censordf$w48), size = 1.2, color = blue) +
  annotate("text", x = 75, y = .06, label = "Mean Censored", color = blue) +
  geom_vline(xintercept = mean(vldf$w48), size = 1.2, color = green) +
  annotate("text", x = 40, y = .04, label = "Mean True", color = green)
ggcensor
```

As you can see from the table and the density graph, the mean of the censored viral loads at 48 weeks is more than twice the true value as is the median value. That is because all values less than 50 were removed from the dataset because we did not know their true value; they were "BDL". The censorship strategy leaves a highly biased dataset. The variance is greatly reduced, approximately a third the variance of the true values. Again, this is due to censoring–eliminating values within a certain range. Here the eliminated range is from the lowest value among the true values (`r  min(vldf$w48)`) up to 49.

### Treat as 0

This is a very attractive and tempting option for any research team. The objective of HIV treatment and cure projects is to eliminate the virus, drive it to zero. It would be very tempting to simply treat any viral load value that cannot be specified for being below the detection limit is to treat it as indistinguishable from zero. This has support in many cases by assuming that someone whose viral load is BDL is no longer communicable. The tendency is to assume that if the viral load cannot be counted, it is because it is not there. However, we know that is not the case. Below Detection Limit does not substantively translate to a value of zero. So, this strategy as well comes with conceptual as well as calculation issues that argue against implementing it.

However, we need to test the implications of this strategy in terms of bias and variance as well. In this strategy, we will take all those values that are "bdl" in the dataset and substitute 0 for them.

```{r}

#| label: go_to_0

go_zero <- vldf |> 
  mutate(zero_w48 = ifelse(w48_bdl == "bdl", 0, as.numeric(w48_bdl)))

wk48_compare <- wk48_compare |> 
  add_case(strategy = "Zero",
           mean_wk48 = mean(go_zero$zero_w48),
           diff = mean_wk48 - true_mean,
           sd_wk48 = sd(go_zero$zero_w48),
           var_wk48 = sd_wk48^2,
           med_wk48 = median(go_zero$zero_w48),
           num = nrow(go_zero), 
           cv = sd_wk48/mean_wk48)

compare_strat_table()

gg_gozero  <- tibble(true = vldf$w48,
                     gozero = go_zero$zero_w48) |> 
  pivot_longer(cols = everything(), names_to = "Strategy", values_to = "vl")|> 
  ggdensity(x = "vl",
            palette = "uchicago",
            color = "Strategy",
            fill = "Strategy",
            xlab = "Viral Load",
            ylab = "Density",
            title = "Density of Zeroed Viral Load",
            xlim = c(0, 110),
            ggtheme = theme_bw()) +
  geom_vline(xintercept = mean(go_zero$zero_w48), size = 1.2, color = blue) +
  annotate("text", x = 20, y = .06, label = "Mean Zeroed", color = blue) +
  geom_vline(xintercept = mean(vldf$w48), size = 1.2, color = green) +
  annotate("text", x = 40, y = .04, label = "Mean True", color = green)
gg_gozero
```

Here, the mean of the distribution now reflects the `r num_vals_bdl` cases that have viral loads that cannot be accurately measured. Although we know that if we had been able to measure these values, they would have produced a mean of `r round(wk48_compare$mean_wk48[1],2)` and a variance of `r round(wk48_compare$var_wk48[1],2)`. However, because of the huge numbers of false zeros that have been inserted, the mean has now fallen to `r round(wk48_compare$mean_wk48[3],2)`. Because there are now the mass of zero values and a lesser number of high values, with few in the middle (the trough of the graph on the density graph), the variance has increased to `r round(wk48_compare$var_wk48[3],2)` . Since the zero values represent such a large portion of the dataset, the median has fallen to 0.

Thus, while tempting, this strategy also distorts dramatically the true nature of the data and bases medical and research decision-making on a false premise.

### Treat as Detection Limit

Just as the prior strategy was optimistic, this strategy is pessimistic. In this strategy, we set all the BDL values to the detection limit itself, which we are setting at `r dl` copies for this analysis. However, it is similar in that it chooses a single, arbitrary value for all the BDL cases.

This too distorts the underling true distribution of viral loads.

```{r}

#| label: dl

go_dl <- vldf |> 
  mutate(dl_w48 = ifelse(w48_bdl == "bdl", dl, as.numeric(w48_bdl)))

wk48_compare <- wk48_compare |> 
  add_case(strategy = "Detection Limit",
           mean_wk48 = mean(go_dl$dl_w48),
           diff = mean_wk48 - true_mean,
           sd_wk48 = sd(go_dl$dl_w48),
           var_wk48 = sd_wk48^2,
           med_wk48 = median(go_dl$dl_w48),
           num = nrow(go_dl), 
           cv = sd_wk48/mean_wk48)

compare_strat_table()

gg_godl  <- tibble(true = vldf$w48,
                   detect_limit = go_dl$dl_w48) |> 
  pivot_longer(cols = everything(), names_to = "Strategy", values_to = "vl")|> 
  ggdensity(x = "vl",
            palette = "uchicago",
            color = "Strategy",
            fill = "Strategy",
            xlab = "Viral Load",
            ylab = "Density",
            title = "Density of Detection Limit Viral Load",
            xlim = c(0, 110),
            ggtheme = theme_bw()) +
  geom_vline(xintercept = mean(go_dl$dl_w48), size = 1.2, color = blue) +
  annotate("text", x = 68, y = .21, label = "Mean Detec. Limit", color = blue) +
  geom_vline(xintercept = mean(vldf$w48), size = 1.2, color = green) +
  annotate("text", x = 20, y = .16, label = "Mean True", color = green)
gg_godl
```

As with the censoring strategy, substituting the detection limit for BDL pushes the distribution of viral loads into a narrow range around that detection limit. As can be seen in the table above, the mean and the median are very close together and the variance now is less than half that of censored strategy, the lowest previous value. While the mean of this strategy is closer to the mean of the true values than the censoring strategy (a difference of `r round(wk48_compare$diff[4], 2)` copies versus `r round(wk48_compare$diff[2], 2)` copies), the concentration of values around `r dl` copies limits the accuracy and utility of this strategy.

### Treat as Half of the Detection Limit

The final of these strategies that I have come across is that of treating the BDL values as one-half of the detection limit, in this case `r dl/2`. I'm not sure what the justification for this is beyond being a specific value somewhere between zero and the detection limit. As with the other strategies, this arbitrariness leads to considerable bias in calculations and then inferences related to viral load.

```{r}

#| label: half_dl

half_dl <- vldf |> 
  mutate(dl_w48 = ifelse(w48_bdl == "bdl", dl/2, as.numeric(w48_bdl)))

wk48_compare <- wk48_compare |> 
  add_case(strategy = "Half Detection Limit",
           mean_wk48 = mean(half_dl$dl_w48),
           diff = mean_wk48 - true_mean,
           sd_wk48 = sd(half_dl$dl_w48),
           var_wk48 = sd_wk48^2,
           med_wk48 = median(half_dl$dl_w48),
           num = nrow(half_dl), 
           cv = sd_wk48/mean_wk48)

compare_strat_table()

gg_half_dl  <- tibble(true = vldf$w48,
                   half_detect_limit = half_dl$dl_w48) |> 
  pivot_longer(cols = everything(), names_to = "Strategy", values_to = "vl")|> 
  ggdensity(x = "vl",
            palette = "uchicago",
            color = "Strategy",
            fill = "Strategy",
            xlab = "Viral Load",
            ylab = "Density",
            title = "Density of Half Detection Limit Viral Load",
            xlim = c(0, 110),
            ggtheme = theme_bw()) +
  geom_vline(xintercept = mean(half_dl$dl_w48), size = .8, color = blue) +
  annotate("text", x = 51, y = .16, label = "Mean Half Detec. Limit", color = blue) +
  geom_vline(xintercept = mean(vldf$w48), size = .8, color = green) +
  annotate("text", x = 20, y = .14, label = "Mean True", color = green)
gg_half_dl

```

Empirically, this strategy does a relatively good job of estimating the distribution of the true values of the viral load distribution. The difference between the mean under this strategy and the true mean is very small (`r round(wk48_compare$diff[5])`). As with all the strategies that focus on a single value as a substitute for BDL, the density of the distribution is focused very narrowly around the half mean value with the only distant cluster being those viral loads that were greater than `r dl` initially. These appear as the concentration whose center is just below 60. This works well as the `r numbdl` values that are classed as BDL actually have a mean of `r round(mean(vldf$w48[vldf$w48 < 50]), 2)` , a value also extremely close to the assigned value of `r dl/2`. As the detection limit declines, this approximation will become even closer. So, in terms of distortion of the data, this strategy offers an acceptable practical solution.

## Imputation of Values Below Detection Limit

All of these four strategies have the defect of substituting a range of values by a single value, losing the variance inherent in what I have called here the true values. We are considering a single variable, isolated from covariates that describe the phenotype of the person whose viral load we are estimating. If we had a set of variables, we could estimate values for the censored BDL values using one of a number of systems of multiple imputation, such as multiple regression imputation, or multiple imputation by chained equations (MICE)[^3]. [@buuren2011]

[^3]: In a later post, I will address some of these techniques applied to the problem of the machine limits in determining values for viral load and other assays of HIV-1.

However, in the case that we simply have viral load to consider, the only way we can "fill in the blanks" of viral loads below detection limit is to determine the distribution that describes the data and estimate the parameters of that distribution. Knowing these quantities, we can estimate a series of random values to stand in for the missing BDL values. The process of estimating which of the many known statistical distributions the empirical data is closest to is well known. Various distribution estimating algorithms are available in R. In this discussion, I will focus on fitdistrplus which uses maximum likelihood (mle) optimization to settle on a set of parameters that best fit the empirical data provided[^4].[@delignette-muller2015]

[^4]: mle is not the only optimization algorithm available in fitdistrplus. However, it is the one I will focus on here.

::: callout-important
Remember that although we know the true values of our artificial dataset that are below the detection limit of `r dl`, with real data, we will only have NA, BDL, or other indication that the machine could not produce a reliable value. So, we would be limited to working with only those values above the detection limit.
:::

### Determining the Distribution of the Original Values

The process of choosing a "best fitting" distribution involves testing the fit of various known distributions (such as normal, uniform, gamma, etc.) against the known values of the dataset. To prepare for this, it would be useful to plot the values above `r dl` to see if it suggests any of the typical curves of distributions.

```{r}

#| label: adl_curve

adl  <- as_tibble(vldf$w48[vldf$w48 >= 50])

adl |> 
  ggdensity(x = "value",
            color = crimson,
            fill = crimson,
            xlab = "Viral Load",
            ylab = "Density",
            title = "Density Curve of Viral Load",
            subtitle = "Above Detection Limit",
            add = "mean",
            add.params = list(color = blue, linetype = "solid"),
            ggtheme = theme_bw()) +
  annotate("text", x = 58, y = .16, label = "Mean", color = blue)

skewness <- psych::skew(adl$value)

```

While this form shows a single peak that resembles a normal distribution, it is skewed to the right (skewness = `r skewness`). This makes it not directly a normal distribution. The Gamma distribution is a family of distributions whose shape is determined by two parameters. Among the possible curves of the distribution are a number that appear similar to our viral load data. For our analysis, we will test the Normal, Gamma, and Uniform distributions. I am adding the Uniform distribution to our inquiry list so we can see what a distribution that definitely doesn't fit the data looks like[^5].

[^5]: The Uniform distribution doesn't fit the data as it assumes that the probability that the variable will assume any individual value is equal to the probability of any other value. Therefore, no peaks possible. This is clearly not the case for our data.

```{r}

#| label: fitting distributions

library(fitdistrplus)

# normal dist
fit_norm <- fitdist(adl$value, "norm")

# gamma dist
fit_gamma <- fitdist(adl$value, "gamma")

# uniform dist
fit_unif <- fitdist(adl$value, "unif")

# comparative fit
gof <- gofstat(list(fit_norm, fit_gamma, fit_unif), 
               fitnames = c("Normal", "Gamma", "Uniform"))
# table of stats
gof_df <- tibble(dist = c("Normal", "Gamma", "Uniform"),
                 mle = c(fit_norm$loglik, fit_gamma$loglik, fit_unif$loglik),
                 cvm = gof$cvm,
                 aic = gof$aic)
gof_df |> 
  gt() |> 
  cols_label(
    dist = "Distribution",
    mle = "log<br>Likelihood",
    cvm = "Cramer-von Mises<br>Statistic",
    aic = "Akaike's Information<br>Criterion",
    .fn = md
  ) |> 
  tab_header(
    title = "Goodness of Fit of Distributions"
  )

cdfcomp(list(fit_norm, fit_gamma, fit_unif), 
        legendtext = c("Normal", "Gamma", "Uniform"))
denscomp(list(fit_norm, fit_gamma, fit_unif), 
         legendtext = c("Normal", "Gamma", "Uniform"))

```

The table shows three evaluation criteria for the three distributions: Loglikelihood of the distribution fitting the empirical data, the result of the Cramer-von Mises one-sample test[@wikipediacontributors2025], and the Akaike's Information Criterion (AIC).[@motulsky2004] For the Loglikelihood test, the maximum value is what we are seeking. In the other two tests, the minimum value would suggest the best distribution to use.

::: callout-tip
Remember that the Loglikelihood values are all negative, so the maximum value is the one closest to 0.
:::

The graphs very clearly show how far away from the shape of the distribution is the Uniform model. The other two distributions are reasonably close to each other, but the Gamma distribution is slightly better both in the table and on the graphs.

::: callout-note
As a practical matter, I usually focus on the AIC to help me choose among models. Because of the simplicity in its calculation, it is a criterion that makes choosing relative strengths easy. The theory behind it is very much more complex than the calculation of it, but in terms of practical implementation, it makes sense to use.
:::

### Calculating New Values for BDL 

So, we will use the Gamma distribution with the parameters specified by distribution fitting function (`fitdistrplus::fitdist()`) to calculate new values to replace those `r numbdl` values that are BDL. We will then test that distribution against the true values to see if this imputation of values approximates the true distribution better than the Half Detection Limit strategy.

```{r}

#| label: new_bdl_values

# get parameters of gamma dist
gamma_shape <- fit_gamma$estimate[1]
gamma_rate <-  fit_gamma$estimate[2]

# choose vector of new values between 1 and 50 according to lognorm distribution
set.seed(1946)
values <- rgamma(12000, shape = gamma_shape, rate = gamma_rate)
values <- round(values[values < 50], 0)[1:n]

vldf$w48_new <- numeric(length = n)
for (i in 1:n) {
  vldf$w48_new[i] <- ifelse(vldf$w48_bdl[i] == "bdl", values[i], vldf$w48[i])
}

wk48_compare <- wk48_compare |> 
  add_case(strategy = "Imputed BDL Values",
           mean_wk48 = mean(vldf$w48_new),
           diff = mean_wk48 - true_mean,
           sd_wk48 = sd(vldf$w48_new),
           var_wk48 = sd_wk48^2,
           med_wk48 = median(vldf$w48_new),
           num = nrow(vldf), 
           cv = sd_wk48/mean_wk48)

compare_strat_table()

# density plot
gg_impute  <- tibble(true = vldf$w48,
                     new = vldf$w48_new) |> 
  pivot_longer(cols = everything(), names_to = "type", values_to = "vl")|> 
  ggdensity(x = "vl",
            palette = "uchicago",
            color = "type",
            fill = "type",
            xlab = "Viral Load",
            ylab = "Density",
            title = "Density of True vs Imputed Values",
            xlim = c(0, 110),
            ggtheme = theme_bw()) +
  geom_vline(xintercept = mean(vldf$w48_new), size = .8, color = blue) +
  annotate("text", x = 51, y = .16, label = "Imputed BDL Values", color = blue) +
  geom_vline(xintercept = mean(vldf$w48), size = .8, color = green) +
  annotate("text", x = 20, y = .14, label = "Mean True", color = green)
gg_impute
```

As both the graph and the last line of the table indicate, univariate imputation based on goodness-of-fit of the observed data (i.e, the data above detection limit) is not a good approximation of the true values originally shown. It is considerably worse than the Half Detection Limit strategy and involves much more work to achieve any result. As stated above, if the viral load or other variable subject to detection limits is part of a larger model with other variates that describe the clinical and virological state of the individuals, then MICE or another multiple imputation strategy may be desirable. This I will address in a later post.
