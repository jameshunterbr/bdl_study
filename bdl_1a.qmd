---
title: "Tests of BDL Bias and Variance - v.1a"
author:
  - name: James R. Hunter, D.Sc.
    affiliation: Laboratório de Retrovirologia, UNIFESP
date: last-modified

format: 
  typst: 
    mainfont: "Nunito"
    fontsize: 11pt

editor: visual

execute:
  echo: false
  warning: false
  message: false
bibliography: references.bib
---

## Purpose

This file will test the determination of bias and variance when calculating viral loads and other variables that are subject to detection limits. For HIV-1 patients, the objective of antiretroviral treatment is to make their plasma RNA viral load zero.

However, there is a practical limit below which the assays cannot distinguish between the presence of virus and having no virus (the zero objective). This is called the "detection limit" and patients who achieve this level are said to have a viral load "BDL", below detection limit. One of the most important consequences of exactitude in measuring true lack of the presence of virus is the phrase "undetectable = untransmittable" [@chang2025][@combescure2009]. This frees, at least emotionally, the individual from concern about transmitting the virus to a partner.

There have been various strategies to deal with calculations of viral load in a panel of patients. One has been to eliminate the cases from the calculation, another has been to treat the viral level of those patients who are BDL as 0, assuming they are really without any virus. Still another strategy has been to assign a value equal to the detection limit. Others have used a value of half the detection limit, i.e., half of the difference between the detection limit and zero.

| Name   | Strategy                         | Value Assigned to "BDL" |
|--------|----------------------------------|-------------------------|
| censor | eliminate case                   | no value                |
| zero   | treat as zero                    | 0                       |
| dl     | treat as detection limit         | detection limit         |
| halfdl | treat as half of detection limit | detection limit/2       |

: Table 1: Strategies for Handling BDL Values

This study was inspired by a blog post by Nikolas Siccha, "Lower Limits of Detection or Quantification".[@siccha2025] He develops a Bayesian model to explore the pharmacodynamics of drugs that cause the measurement of interest to fall below detection limits. As it deals with the dynamics over time of a drug's presence and efficacy in a patient, his question is more complex than the question I would like to address here.

::: callout-important
What is the relationship between the true value of viral load below detection limit and the strategies used to estimate it and how does that effect analyses of the viral load of a sample or population?
:::

```{r}

#| label: packages_variables

#| echo: false

pacman::p_load(tidyverse, janitor, ggpubr, ggsci, gt, summarytools)

uchicago <- pal_uchicago(palette = "default")(8)
crimson <- uchicago[7]
gray <- uchicago[2]
blue <- uchicago[5]
green <- uchicago[8]
stats <- c("mean", "sd", "min", "med", "max", "iqr", "cv")

## assumed values
n <- 1000
blmean <- 900
blsd <- 1000
reduct_pct <- .975 # pct reduction in w48
dl <- 50 # detection limit

```

## Method

I will create a panel of 1,000 PLWH artificially with estimates of their viral load at a baseline and after 48 simulated weeks of antiretroviral treatment (ART) when some of them will have viral loads reduced to BDL, which I will define here as 50 copies per mL.[^1] According to a 2009 clinical trial (STARTMRK) of Raltegravir, a 1st generation integrase inhibitor, 86.1% of the RTG arm of naive patients had viral loads BDL by week 48. [@lennox2009] Likewise, a clinical trial (SINGLE) of a second generation integrase inhibitor, Dolutagravir, showed 88% of the DTG arm with viral loads below detection limit by week 48. [@walmsley2013] For my panel, I will use a proportion of 85% of patients achieving true viral loads between 0 and 50 by the 48 week mark. 85% is a bit more conservative than either of the real trials, but consistent with their results.

[^1]: While 50 copies was a common level for detection limit until 2024, since then it has been reduced to 30 or 20 copies depending on the measurement technology used. I am using 50 here as the purpose is to explore the bias and variance of samples rather than to test the detection limit itself.

The baseline viral loads will be determined by calculating 1,000 normal variates from a truncated normal distribution with a mean of `r format(blmean, big.mark = ",")` copies per mL and a standard deviation of `r format(blsd, big.mark = ",")` copies. Since values below detection limit make no sense for this simulation, these values will be removed by using the truncated normal distribution.[@mersmann2023]

The 48 week data will be reduced dramatically (and unrealistically)[^2] by `r 100 * reduct_pct`% and the number of values below 50 copies per mL will be calculated. I will then create a second version substituting the true values below the detection limit with BDL. Using the four strategies outlined in Table 1, I will then compare the bias and variance of each of these strategies for substituting the true value of the viral load. Finally, I will propose a method for imputing values that is commonly applied in studies using the R statistical language.

[^2]: While unrealistic, this large reduction is useful to give us a number of values BDL that can be tested against the 4 strategies.

The flow chart below shows a schematic of the workflow of this study.

![](flowchart%20bdl.png){fig-align="center" width="3.1in"}

## Baseline

```{r}

#| label: determine_cohort_bl
#| echo: false

set.seed(42)

# calculate baseline distribution
# set as integer
bl <- round(truncnorm::rtruncnorm(n, mean = blmean, sd = blsd, a = dl), 0)

# calculate summary stats
descr(bl, stats = stats)

vldf <- tibble(bl)
vldf |> 
  gghistogram(x = "bl",
              bins = 20,
              fill = crimson,
              title = "Viral Load Baseline Values - Simulated",
              add = "mean",
              add.params = list(color = gray, size = 1.2),
              xlab = "Viral Load",
              ylab = "Count",
              ggtheme = theme_bw()) +
  annotate("text", x = 1800, y = 125, label = "Mean of Viral Loads", 
           color = gray)
```

## Week 48 Viral Load

We will calculate the viral load at 48 weeks to be `r 100 * reduct_pct`% less than the original viral load.

```{r}

#| label: 48_weeks

vldf <- vldf |> 
  mutate(w48 = round(bl * (1 - reduct_pct), 0))

num_vals_bdl <- length(vldf$w48[vldf$w48 < 50])

# substitute "bdl" for values less than 50
vldf <- vldf |>
  mutate(w48_bdl = ifelse(w48 < 50, "bdl", w48))

descr(vldf$w48, stats = stats)

vldf |> 
  gghistogram(x = "w48",
              bins = 20,
              fill = crimson,
              title = "Viral Load Week 48 Values",
              add = "mean",
              add.params = list(color = gray, size = 1.2),
              xlab = "Viral Load",
              ylab = "Count",
              ggtheme = theme_bw()) +
  annotate("text", x = 50, y = 110, label = "Mean of Viral Loads", 
           color = gray)
# tibble to compare true values with strategies
wk48_compare <- vldf |> 
  summarise(
    strategy = "true",
    mean_wk48 = mean(w48),
    sd_wk48 = sd(w48),
    var_wk48 = sd_wk48^2,
    med_wk48 = median(w48),
    num = n())
```

To give you an idea of what we have now, here are the first 10 cases in our dataset. While we know the true values of the viral load, the assay itself will only give us a BDL signal instead of the value. This occurs `r num_vals_bdl` times out of the 1,000 cases. The question is what can we do about it and what does that do to the distribution of viral loads that we are trying to use to make an inference about HIV-1 treatment strategies.

```{r}

#| data_table
#| echo: false

vldf |> 
  slice(1:10) |> 
  gt() |> 
  fmt_number(
    decimals = 0
  ) |> 
  cols_label(
    bl = "Baseline",
    w48 = "True Value",
    w48_bdl = "BDL"
  ) |> 
  tab_spanner(
    label = "Week 48",
    columns = c(w48, w48_bdl)
  )

# store number of bdl's for text below
numbdl <- length(vldf$w48_bdl[vldf$w48_bdl == "bdl"])

vldf |> 
  ggdensity(x = "w48",
            fill = crimson,
            xlab = "Viral Load",
            ylab = "Density",
            title = "Density of True Value Viral Load",
            add = "mean",
            add.params = list(size = 1.2, color = "black"),
            xlim = c(0, 110),
            ggtheme = theme_bw()) +
  annotate("text", x = 40, y = .04, label = "Mean True", color = "black")
```

## Bias and Variance

The measures I am going to use to test the strategies listed at the beginning of this analysis are bias and variance. Bias measures how much the strategy distorts the distribution of true values. I will show this by comparing the mean, median, and standard deviation of the results of the strategies to those of the true values and through a density visualization of the distributions. The variance is rather more straightforward as we can find the statistical variance as determined by the classical formula for this quantity.

## The Four Strategies

### Censorship

In this strategy, which is very commonly applied when laboratories confront results of assays that do not indicate numerical values, simply removes those cases from the study.

When we do that here, we throw away `r numbdl` cases, meaning `r 100 * numbdl/n`% of the cases, which likely contain many other nuggets of useful information. Before we even arrive at judging bias and variance, this reckless disposal of worthwhile information is the principal defect of the strategy. This dataset is not arbitrarily skewed toward having cases with BDL viral load after 48 weeks. As pointed out at the beginning, many studies show results in the range of 85%.

```{r}

#| label: censorship

censordf <- vldf |> 
  filter(w48_bdl != "bdl")

wk48_compare <- wk48_compare |> 
  add_case(strategy = "censored",
           mean_wk48 = mean(censordf$w48),
           sd_wk48 = sd(censordf$w48),
           var_wk48 = sd_wk48^2,
           med_wk48 = median(censordf$w48),
           num = nrow(censordf))

wk48_compare |> 
  gt() |> 
  cols_label(
    strategy = "Strategy",
    mean_wk48 = "Mean",
    sd_wk48 = "Std. Dev.",
    var_wk48 = "Variance",
    med_wk48 = "Median",
    num = "# of Cases"
  ) |> 
  fmt_number(
    columns = c(mean_wk48, sd_wk48, var_wk48),
    decimals = 2
  ) |> 
  tab_spanner(
    label = "Week 48 Viral Load",
    columns = c(mean_wk48:num)
  )

ggcensor <- tibble(true = vldf$w48,
                   censored = c(censordf$w48, rep(NA, num_vals_bdl))) |> 
  pivot_longer(cols = everything(), names_to = "Strategy", values_to = "vl")|> 
  ggdensity(x = "vl",
            palette = "uchicago",
            color = "Strategy",
            fill = "Strategy",
            xlab = "Viral Load",
            ylab = "Density",
            title = "Density of Censored Viral Load",
            xlim = c(0, 110),
            ggtheme = theme_bw()) +
  geom_vline(xintercept = mean(censordf$w48), size = 1.2, color = blue) +
  annotate("text", x = 75, y = .06, label = "Mean Censored", color = blue) +
  geom_vline(xintercept = mean(vldf$w48), size = 1.2, color = green) +
  annotate("text", x = 40, y = .04, label = "Mean True", color = green)
ggcensor
```

As you can see from the table and the density graph, the mean of the censored viral loads at 48 weeks is more than twice the true value as is the median value. That is because all values less than 50 were removed from the dataset because we did not know their true value; they were "BDL". The censorship strategy leaves a highly biased dataset. The variance is greatly reduced, approximately a third the variance of the true values. Again, this is due to censoring–eliminating values within a certain range. Here the eliminated range is from the lowest value among the true values (`r  min(vldf$w48)`) up to 49.

### Treat as 0

This is a very attractive and tempting option for any research team. The objective of HIV treatment and cure projects is to eliminate the virus, drive it to zero. It would be very tempting to simply treat any viral load value that cannot be specified for being below the detection limit is to treat it as indistinguishable from zero. This has support in many cases by assuming that someone whose viral load is BDL is no longer communicable. The tendency is to assume that if the viral load cannot be counted, it is because it is not there. However, we know that is not the case. Below Detection Limit does not substantively translate to a value of zero. So, this strategy as well comes with conceptual as well as calculation issues that argue against implementing it.

However, we need to test the implications of this strategy in terms of bias and variance as well. In this strategy, we will take all those values that are "bdl" in the dataset and substitute 0 for them.

```{r}

#| label: go_to_0

go_zero <- vldf |> 
  mutate(zero_w48 = ifelse(w48_bdl == "bdl", 0, as.numeric(w48_bdl)))

wk48_compare <- wk48_compare |> 
  add_case(strategy = "Zero",
           mean_wk48 = mean(go_zero$zero_w48),
           sd_wk48 = sd(go_zero$zero_w48),
           var_wk48 = sd_wk48^2,
           med_wk48 = median(go_zero$zero_w48),
           num = nrow(go_zero))

wk48_compare |> 
  gt() |> 
  cols_label(
    strategy = "Strategy",
    mean_wk48 = "Mean",
    sd_wk48 = "Std. Dev.",
    var_wk48 = "Variance",
    med_wk48 = "Median",
    num = "# of Cases"
  ) |> 
  fmt_number(
    columns = c(mean_wk48, sd_wk48, var_wk48),
    decimals = 2
  ) |> 
  tab_spanner(
    label = "Week 48 Viral Load",
    columns = c(mean_wk48:num)
  )

gg_gozero  <- tibble(true = vldf$w48,
                     gozero = go_zero$zero_w48) |> 
  pivot_longer(cols = everything(), names_to = "Strategy", values_to = "vl")|> 
  ggdensity(x = "vl",
            palette = "uchicago",
            color = "Strategy",
            fill = "Strategy",
            xlab = "Viral Load",
            ylab = "Density",
            title = "Density of Zeroed Viral Load",
            xlim = c(0, 110),
            ggtheme = theme_bw()) +
  geom_vline(xintercept = mean(go_zero$zero_w48), size = 1.2, color = blue) +
  annotate("text", x = 20, y = .06, label = "Mean Zeroed", color = blue) +
  geom_vline(xintercept = mean(vldf$w48), size = 1.2, color = green) +
  annotate("text", x = 40, y = .04, label = "Mean True", color = green)
gg_gozero
```

Here, the mean of the distribution now reflects the `r num_vals_bdl` cases that have viral loads that cannot be accurately measured. Although we know that if we had been able to measure these values, they would have produced a mean of `r wk48_compare$mean_wk48[1]` and a variance of `r wk48_compare$var_wk48[1]`. However, because of the huge numbers of false zeros that have been inserted, the mean has now fallen to `wk48_compare$mean_wk48[3]`. Because there are now the mass of zero values and a lesser number of high values, with few in the middle (the trough of the graph on the density graph), the variance has increased to `r wk48_compare$var_wk48[3]` . Since the zero values represent such a large portion of the dataset, the median has fallen to 0.

Thus, while tempting, this strategy also distorts dramatically the true nature of the data and bases medical and research decision-making on a false premise.
